1.相应地设置每个服务器的主机名，并在所有Ceph集群节点上配置/etc/hosts
10.28.5.249 	ceph-admin
10.28.5.244 	ceph-mon01
10.28.5.231 	ceph-mon02
10.28.5.247 	ceph-mon03
10.28.5.241	    ceph-rgw01
10.28.5.246  	ceph-rgw02
10.28.5.240 	ceph-osd01
10.28.5.230 	ceph-osd02
10.28.5.233 	ceph-osd03
10.28.5.232 	ceph-osd04

2.确保系统已更新：
sudo apt update
sudo apt upgrade
sudo reboot

3.准备Ceph管理节点
wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -
echo deb https://download.ceph.com/debian-mimic/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
sudo apt update
sudo apt -y install ceph-deploy

4.准备Ceph节点,所有节点都必须配置,确保无密访问
export USERNAME="ceph-admin"
export USER_PASS="1q2w@WSX"
useradd --create-home -s /bin/bash ${USERNAME}
echo "${USERNAME}:${USER_PASS}"|chpasswd
echo "${USERNAME} ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/${USERNAME}
chmod 0440 /etc/sudoers.d/${USERNAME}
su - ceph-admin
ssh-keygen
配置~/.ssh/config：
Host ceph-osd01
Hostname ceph-osd01
User ceph-admin
Host ceph-osd02
Hostname ceph-osd02
User ceph-admin
Host ceph-osd03
Hostname ceph-osd03
User ceph-admin
Host ceph-osd04
Hostname ceph-osd04
User ceph-admin
Host ceph-mon01
Hostname ceph-mon01
User ceph-admin
Host ceph-mon02
Hostname ceph-mon02
User ceph-admin
Host ceph-mon03
Hostname ceph-mon03
User ceph-admin
Host ceph-rgw01
Hostname ceph-rgw01
User ceph-admin
Host ceph-rgw02
Hostname ceph-rgw02
User ceph-admin

5. 将密钥复制到每个Ceph节点（从Ceph管理节点执行此操作作为ceph-admin用户):

for i in ceph-rgw01 ceph-rgw02 ceph-mon01 ceph-mon02 ceph-mon03 ceph-osd01 oceph-sd02 ceph-osd03 ceph-osd04; do
ssh-copy-id $i
done

6.确保在所有Ceph节点上设置NTP
apt install ntp -y

7.在管理节点上创建一个目录，用于维护ceph-deploy为集群生成的配置文件和密钥
mkdir ceph-deploy && cd ceph-deploy

8. 初始化ceph监控节点 (apt install -y python2.7)
ceph-deploy new ceph-mon01 ceph-mon02 ceph-mon03

9. 安装Ceph包
ceph-deploy install ceph-rgw01 ceph-rgw02 ceph-mon01 ceph-mon02 ceph-mon03 ceph-osd01 ceph-osd02 ceph-osd03 ceph-osd04

10.部署初始监视器并收集密钥
ceph-deploy mon create-initial

11.将ceph配置文件和密钥环复制到节点
ceph-deploy admin ceph-osd01 ceph-osd02 ceph-osd03 ceph-osd04

12.部署管理器守护程序
ceph-deploy mgr create ceph-osd03

13.添加OSD
ceph-osd03
---------
for i in sdb sdc sdd sde sdf sdg sdh sdi sdj sdk ; do 
   echo "ceph-deploy osd create $i"
   ceph-deploy osd create --bluestore --data /dev/$i ceph-osd03
   echo "osd create over."
done

ceph-osd04
---------
for i in sdb sdc sdd sde sdf sdg sdh sdi sdj sdk ; do 
   echo "ceph-deploy osd create $i"
   ceph-deploy osd create --bluestore --data /dev/$i ceph-osd04
   echo "osd create over."
done

ceph-osd01
---------
for i in sdb sdc sdd sde sdf sdg sdh sdi sdj sdk ; do 
   echo "ceph-deploy osd create $i"
   ceph-deploy osd create --data /dev/$i ceph-osd01
   echo "osd create over."
done

ceph-osd02
---------
for i in sdb sdc sdd sde sdf sdg sdh sdi sdj sdk ; do 
   echo "ceph-deploy osd create $i"
   ceph-deploy osd create --data /dev/$i ceph-osd02
   echo "osd create over."
done

14. 检查集群是否健康,登录到osd节点 ceph -s

15. 添加网关节点
ceph-deploy rgw create  ceph-rgw01 ceph-rgw02

16. 创建冷热存储池
a. 查看class类型
ceph osd crush class ls
b. 删除默认的class
for i in {0..19} ; do ceph osd crush rm-device-class osd.$i;done
c. 标记SSD
for i in {0..19};do ceph osd crush set-device-class ssd osd.$i;done
d. 创建ssd规则
ceph osd crush rule create-replicated rule-ssd default  host ssd
ceph osd crush rule create-replicated rule-hdd default  host hdd
e. 检查
ceph osd crush rule ls
f. 创建热数据存储池
ceph osd pool create hot-storage 2048 2048 replicated rule-ssd 40960
g. 创建冷数据存储池
ceph osd pool create cold-storage 2048 2048 replicated rule-hdd 409600
h. 分级缓存
设置一缓存层需把缓存存储池挂接到后端存储池上
ceph osd tier add cold-storage hot-storage
设置缓存模式
ceph osd tier cache-mode hot-storage writeback
缓存层盖在后端存储层之上
ceph osd tier set-overlay cold-storage hot-storage
配置缓存层
ceph osd pool set hot-storage hit_set_type bloom
ceph osd pool set hot-storage hit_set_count 24
ceph osd pool set hot-storage hit_set_period 3600
ceph osd pool set hot-storage target_max_bytes 800000000000
ceph osd pool set hot-storage min_read_recency_for_promote 8
ceph osd pool set hot-storage min_write_recency_for_promote 8
ceph osd pool set hot-storage cache_target_dirty_ratio 0.4
ceph osd pool set hot-storage cache_target_dirty_high_ratio 0.6
ceph osd pool set hot-storage cache_target_full_ratio 0.8
ceph osd pool set hot-storage target_max_bytes 800000000000
ceph osd pool set hot-storage target_max_objects 1000000
ceph osd pool set hot-storage cache_min_flush_age 600
ceph osd pool set hot-storage cache_min_evict_age 1800

17. 压测

rados bench -p hot-storage 15 -t 8 write  --no-cleanup
rados bench -p hot-storage 15 -t 8 seq 
rados bench -p hot-storage 15 -t 8 rand 
rados -p hot-storage cleanup

rados bench -p cold-storage 15 -t 8 write --no-cleanup
rados bench -p cold-storage 15 -t 8 seq 
rados bench -p cold-storage 15 -t 8 rand 
rados -p cold-storage cleanup

ceph osd pool create ankr-backup 512 512 replicated rule-hdd 409600
rados bench -p ankr-backup 15 -t 8 write --no-cleanup
rados -p ankr-backup cleanup

ceph osd pool create fast-storage 256 256 replicated rule-ssd 409600
rados bench -p fast-storage 15 -t 8 write --no-cleanup
rados -p fast-storage cleanup

18.配置k8s存储类
mix:
kubectl patch sc managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

kubectl patch sc managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

kubectl patch sc mix -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

kubectl patch sc managed-nfs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=cold-storage'
ceph auth get-key client.kube | base64
fast:


-------------------------------------------------------------------------------------------------
重置集群
1. ceph-deploy purge ceph-rgw01 ceph-rgw02 ceph-mon01 ceph-mon02 ceph-mon03 ceph-osd01 ceph-osd02 ceph-osd03 ceph-osd04
2. ceph-deploy purgedata ceph-rgw01 ceph-rgw02 ceph-mon01 ceph-mon02 ceph-mon03 ceph-osd01 ceph-osd02 ceph-osd03 ceph-osd04
3. ceph-deploy forgetkeys
4. rm ceph.*




